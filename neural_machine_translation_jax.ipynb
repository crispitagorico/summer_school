{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbX8ebIHOa_c"
      },
      "source": [
        "Please find torch implementation of this notebook here: https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/15/nmt_torch.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks-d2l/nmt_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aExsxPtwcY8v"
      },
      "source": [
        "# Neural machine translation using encoder-decoder RNN\n",
        "\n",
        "We show how to implement NMT using an encoder-decoder.\n",
        "\n",
        "Based on sec 9.7 of http://d2l.ai/chapter_recurrent-modern/seq2seq.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF-NgxfLcYPn",
        "outputId": "b9dd7bd8-2e38-4fc8-9d93-af2e16f74b89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘figures’: File exists\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "from IPython import display\n",
        "\n",
        "import jax\n",
        "\n",
        "try:\n",
        "    import flax.linen as nn\n",
        "except ModuleNotFoundError:\n",
        "    %pip install -qq flax\n",
        "    import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "\n",
        "try:\n",
        "    import optax\n",
        "except ModuleNotFoundError:\n",
        "    %pip install -qq optax\n",
        "    import optax\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import hashlib\n",
        "import time\n",
        "import functools\n",
        "\n",
        "random.seed(0)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "!mkdir figures # for saving plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF_JE7xaZKRn"
      },
      "source": [
        "# Required functions for text preprocessing\n",
        "\n",
        "For more details on this functions: See [this colab](https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/text_preproc_torch.ipynb#scrollTo=yDmK1xQ9T4IY) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "o_C6Ke82BM7V"
      },
      "outputs": [],
      "source": [
        "# Required functions for downloading data\n",
        "\n",
        "\n",
        "def download(name, cache_dir=os.path.join(\"..\", \"data\")):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split(\"/\")[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, \"rb\") as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f\"Downloading {fname} from {url}...\")\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "\n",
        "def download_extract(name, folder=None):\n",
        "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
        "    fname = download(name)\n",
        "    base_dir = os.path.dirname(fname)\n",
        "    data_dir, ext = os.path.splitext(fname)\n",
        "    if ext == \".zip\":\n",
        "        fp = zipfile.ZipFile(fname, \"r\")\n",
        "    elif ext in (\".tar\", \".gz\"):\n",
        "        fp = tarfile.open(fname, \"r\")\n",
        "    else:\n",
        "        assert False, \"Only zip/tar files can be extracted.\"\n",
        "    fp.extractall(base_dir)\n",
        "    return os.path.join(base_dir, folder) if folder else data_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-c0dzTV5AXXn"
      },
      "outputs": [],
      "source": [
        "def read_data_nmt():\n",
        "    \"\"\"Load the English-French dataset.\"\"\"\n",
        "    data_dir = download_extract(\"fra-eng\")\n",
        "    with open(os.path.join(data_dir, \"fra.txt\"), \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def preprocess_nmt(text):\n",
        "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
        "\n",
        "    def no_space(char, prev_char):\n",
        "        return char in set(\",.!?\") and prev_char != \" \"\n",
        "\n",
        "    # Replace non-breaking space with space, and convert uppercase letters to\n",
        "    # lowercase ones\n",
        "    text = text.replace(\"\\u202f\", \" \").replace(\"\\xa0\", \" \").lower()\n",
        "    # Insert space between words and punctuation marks\n",
        "    out = [\" \" + char if i > 0 and no_space(char, text[i - 1]) else char for i, char in enumerate(text)]\n",
        "    return \"\".join(out)\n",
        "\n",
        "\n",
        "def tokenize_nmt(text, num_examples=None):\n",
        "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
        "    source, target = [], []\n",
        "    for i, line in enumerate(text.split(\"\\n\")):\n",
        "        if num_examples and i > num_examples:\n",
        "            break\n",
        "        parts = line.split(\"\\t\")\n",
        "        if len(parts) == 2:\n",
        "            source.append(parts[0].split(\" \"))\n",
        "            target.append(parts[1].split(\" \"))\n",
        "    return source, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GE_0EjLhH1pR"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.unk, uniq_tokens = 0, [\"<unk>\"] + reserved_tokens\n",
        "        uniq_tokens += [token for token, freq in self.token_freqs if freq >= min_freq and token not in uniq_tokens]\n",
        "        self.idx_to_token, self.token_to_idx = [], dict()\n",
        "        for token in uniq_tokens:\n",
        "            self.idx_to_token.append(token)\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Gbs426U9IQU0"
      },
      "outputs": [],
      "source": [
        "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
        "astype = lambda x, *args, **kwargs: jnp.array(x, *args, **kwargs)\n",
        "\n",
        "\n",
        "def build_array_nmt(lines, vocab, num_steps):\n",
        "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
        "    lines = [vocab[l] for l in lines]\n",
        "    lines = [l + [vocab[\"<eos>\"]] for l in lines]\n",
        "    array = jnp.array([truncate_pad(l, num_steps, vocab[\"<pad>\"]) for l in lines])\n",
        "    valid_len = reduce_sum(astype(array != vocab[\"<pad>\"], jnp.int32), 1)\n",
        "    return array, valid_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PpUa9rqcl0Pz"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    \"\"\"A dataset wrapping data arrays.\"\"\"\n",
        "\n",
        "    def __init__(self, *data_arrays):\n",
        "        self.arrays = data_arrays\n",
        "\n",
        "    def __getitem__(self, indices):\n",
        "        return tuple(array[jnp.array(indices)] for array in self.arrays)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.arrays[0].shape[0]\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"A data loader with an iterable for data arrays.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, batch_size, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.data_iter()\n",
        "\n",
        "    def data_iter(self):\n",
        "        indices = list(range(0, len(self.dataset)))\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(indices)\n",
        "\n",
        "        for i in range(0, len(indices), self.batch_size):\n",
        "            yield self.dataset[indices[i : i + self.batch_size]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3yYUa_itADJm"
      },
      "outputs": [],
      "source": [
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "    \"\"\"Construct a data iterator.\"\"\"\n",
        "    dataset = Dataset(*data_arrays)\n",
        "    return DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "\n",
        "def truncate_pad(line, num_steps, padding_token):\n",
        "    \"\"\"Truncate or pad sequences.\"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps]  # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line))\n",
        "\n",
        "\n",
        "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
        "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
        "    text = preprocess_nmt(read_data_nmt())\n",
        "    source, target = tokenize_nmt(text, num_examples)\n",
        "    src_vocab = Vocab(source, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
        "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
        "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
        "    data_iter = load_array(data_arrays, batch_size)\n",
        "    return data_iter, src_vocab, tgt_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1dieC4GqW68"
      },
      "source": [
        "# Data\n",
        "\n",
        "We use a english-french dataset. See [this colab](https://colab.research.google.com/github/probml/probml-notebooks/blob/main/notebooks/text_preproc_torch.ipynb#scrollTo=yDmK1xQ9T4IY) for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r-4oeZhRqZ2y"
      },
      "outputs": [],
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/\"\n",
        "\n",
        "DATA_HUB[\"fra-eng\"] = (DATA_URL + \"fra-eng.zip\", \"94646ad1522d915e7b0f9296181140edcf86a4f5\")\n",
        "\n",
        "batch_size, num_steps = 64, 10\n",
        "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMzkvJuXcyw5"
      },
      "source": [
        "# Encoder-decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0WQNa-n7TcYt"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"The encoder-decoder architecture.\"\"\"\n",
        "\n",
        "    encoder: nn.Module\n",
        "    decoder: nn.Module\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, enc_X, dec_X, *args, deterministic=True):\n",
        "        enc_outputs = self.encoder(enc_X, *args, deterministic=deterministic)\n",
        "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
        "        return self.decoder(dec_state, dec_X, deterministic=deterministic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftsr8h2CdGoP"
      },
      "source": [
        "## GRU\n",
        "\n",
        "We create a multilayer GRU RNN. The hidden state of a layer, followed by dropout, is the input to the next layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uj5uYpw2ulJv"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\"Multi-layer GRU with dropout, scanned over the time axis.\"\"\"\n",
        "    num_layers: int = 1\n",
        "    dropout_rate: float = 0.0\n",
        "    deterministic: bool = True\n",
        "\n",
        "    @functools.partial(\n",
        "        nn.transforms.scan,\n",
        "        variable_broadcast=\"params\",\n",
        "        in_axes=0,\n",
        "        out_axes=0,\n",
        "        split_rngs={\"params\": False, \"dropout\": True},\n",
        "    )\n",
        "    @nn.compact\n",
        "    def __call__(self, state, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          state: array of shape (num_layers, batch_size, hidden_size)\n",
        "          x:     array of shape (batch_size, input_size)\n",
        "        Returns:\n",
        "          new_state: array of shape (num_layers, batch_size, hidden_size)\n",
        "          output:    array of shape (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        new_states = []\n",
        "\n",
        "        # First num_layers-1 cells with dropout after each\n",
        "        for i in range(self.num_layers - 1):\n",
        "            s, x = nn.GRUCell(features=state.shape[-1])(state[i], x)\n",
        "            new_states.append(s)\n",
        "            x = nn.Dropout(rate=self.dropout_rate)(\n",
        "                x, deterministic=self.deterministic\n",
        "            )\n",
        "\n",
        "        # Final layer, no dropout on its output\n",
        "        s, out = nn.GRUCell(features=state.shape[-1])(state[-1], x)\n",
        "        new_states.append(s)\n",
        "\n",
        "        # stack to shape (num_layers, batch_size, hidden_size)\n",
        "        return jnp.stack(new_states, axis=0), out\n",
        "\n",
        "    def initialize_carry(self, rng, batch_dims, size):\n",
        "        \"\"\"\n",
        "        Zero-initialize the hidden state for all layers,\n",
        "        shape = (num_layers, *batch_dims, hidden_size).\n",
        "        \"\"\"\n",
        "        # dtype matches default GRUCell carry dtype\n",
        "        return jnp.zeros((self.num_layers, *batch_dims, size), dtype=jnp.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J5JnFy_dgU8"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "We use a 2-level GRU for the encoder; we set the context as the final state of the GRU. The input to the GRU is the word embedding of each token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uq--PRcjLBaK"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqEncoder(nn.Module):\n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "\n",
        "    vocab_size: int\n",
        "    embed_size: int\n",
        "    num_hiddens: int\n",
        "    num_layers: int\n",
        "    dropout_rate: float = 0.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, X, *args, deterministic=True):\n",
        "        # The output `X` shape: (`batch_size`, `num_steps`, `embed_size`)\n",
        "        X = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_size)(X)\n",
        "        # In RNN models, the first axis corresponds to time steps\n",
        "        X = X.transpose(1, 0, 2)\n",
        "        rnn = GRU(num_layers=self.num_layers, dropout_rate=self.dropout_rate, deterministic=deterministic)\n",
        "        state = rnn.initialize_carry(jax.random.PRNGKey(0), (X.shape[1],), self.num_hiddens)\n",
        "        state, output = rnn(state, X)\n",
        "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
        "        # `output` shape: (`num_steps`, `batch_size`, `num_hiddens`)\n",
        "        return state, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ895FigmY3A",
        "outputId": "8ef8b101-c114-40e0-a3ab-74d20bcceab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 4, 16)\n",
            "(2, 4, 16)\n"
          ]
        }
      ],
      "source": [
        "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
        "batch_size = 4\n",
        "num_steps = 7\n",
        "X = jnp.zeros((batch_size, num_steps), dtype=jnp.int32)\n",
        "encoder_params = encoder.init(rng, X)\n",
        "state, output = encoder.apply(encoder_params, X)\n",
        "print(output.shape)\n",
        "print(state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twNb8LAJe6So"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "We use another GRU as the decoder. The initial state is the final state of the encoder, so we must use the same number of hidden units. In addition, we pass in the context (ie final state of encoder) as input to every step of the decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7YKxPKssoa9L"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqDecoder(nn.Module):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    vocab_size: int\n",
        "    embed_size: int\n",
        "    num_hiddens: int\n",
        "    num_layers: int\n",
        "    dropout_rate: float = 0.0\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, state, X, deterministic=True):\n",
        "        # X: (batch_size, num_steps)\n",
        "        # 1) embed and transpose to (num_steps, batch_size, embed_size)\n",
        "        X = nn.Embed(num_embeddings=self.vocab_size, features=self.embed_size)(X)\n",
        "        X = X.transpose(1, 0, 2)\n",
        "\n",
        "        # 2) build context by repeating the last encoder hidden across time\n",
        "        #    state[-1] has shape (batch_size, num_hiddens)\n",
        "        #    we add a time axis then tile:\n",
        "        #      state[-1][None, :, :] -> (1, batch_size, num_hiddens)\n",
        "        #      tiled to           -> (num_steps, batch_size, num_hiddens)\n",
        "        num_steps = X.shape[0]\n",
        "        context = jnp.tile(state[-1][None, :, :], (num_steps, 1, 1))\n",
        "\n",
        "        # 3) concatenate inputs and context along feature dim -> (num_steps, batch_size, embed+hidden)\n",
        "        X_and_context = jnp.concatenate([X, context], axis=-1)\n",
        "\n",
        "        # 4) run through our multi-layer GRU (scanned over time)\n",
        "        rnn = GRU(\n",
        "            num_layers=self.num_layers,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            deterministic=deterministic,\n",
        "        )\n",
        "        state, rnn_output = rnn(state, X_and_context)\n",
        "        # rnn_output: (num_steps, batch_size, num_hiddens)\n",
        "\n",
        "        # 5) project to vocabulary and transpose back -> (batch_size, num_steps, vocab_size)\n",
        "        logits = nn.Dense(features=self.vocab_size)(rnn_output)\n",
        "        logits = logits.transpose(1, 0, 2)\n",
        "\n",
        "        return state, logits\n",
        "\n",
        "    def init_state(self, enc_outputs, *args):\n",
        "        # enc_outputs is a tuple (state, output) from the encoder\n",
        "        # we just take the encoder’s final hidden-state array\n",
        "        return enc_outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBpEzLPHqY62",
        "outputId": "2e68d0c7-1f6c-4465-ae71-44800d31c50b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 7, 10)\n",
            "(2, 4, 16)\n"
          ]
        }
      ],
      "source": [
        "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
        "state = decoder.init_state(encoder.apply(encoder_params, X))\n",
        "decoder_params = decoder.init(rng, state, X)\n",
        "state, output = decoder.apply(decoder_params, state, X)\n",
        "print(output.shape)  # (batch size, number of time steps, vocabulary size)\n",
        "print(state.shape)  # (num layers, batch size, num hiddens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfhSjLALgN3R"
      },
      "source": [
        "# Loss function\n",
        "\n",
        "We use cross entropy loss, but we must mask out target tokens that are just padding. We replace all outputs beyond the valid length to the target value of 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIL9P5h3gSW2",
        "outputId": "f51dc337-5bf1-4f45-c1b4-f36ac98d238c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[1, 0, 0],\n",
              "       [4, 5, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "def sequence_mask(X, valid_len, value=0):\n",
        "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
        "    maxlen = X.shape[1]\n",
        "    mask = jnp.arange((maxlen))[None, :] < valid_len[:, None]\n",
        "    return jnp.where(mask, X, value)\n",
        "\n",
        "\n",
        "X = jnp.array([[1, 2, 3], [4, 5, 6]])\n",
        "sequence_mask(X, jnp.array([1, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYKVdTZ9hAKL"
      },
      "source": [
        "We now use this to create a weight mask of 0s and 1s, where 0 corresponds to invalid locations. When we compute the cross entropy loss, we multiply by this weight mask, thus ignoring invalid locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OlHTsKhxg_RM"
      },
      "outputs": [],
      "source": [
        "def masked_softmax_cross_entropy(pred, label, valid_len):\n",
        "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
        "\n",
        "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
        "    # `label` shape: (`batch_size`, `num_steps`)\n",
        "    # `valid_len` shape: (`batch_size`,)\n",
        "    weights = jnp.ones(label.shape)\n",
        "    weights = sequence_mask(weights, valid_len)\n",
        "    label_one_hot = jax.nn.one_hot(label, num_classes=pred.shape[-1])\n",
        "    unweighted_loss = optax.softmax_cross_entropy(pred, label_one_hot)\n",
        "    weighted_loss = (unweighted_loss * weights).mean(axis=1)\n",
        "    return weighted_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UEwUQaphtb5"
      },
      "source": [
        "As an example, let us create a prediction array of all ones of size (3,4,10) and a target label array of all ones of size (3,4). We specify the valid lengths to (4,2,0). Thus the first loss should be twice the second. And the third loss should be 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_zzw7I4wiO0",
        "outputId": "27d93e2b-6de7-466d-aa31-615e2c01cb96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([2.3025851, 1.1512926, 0.       ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "masked_softmax_cross_entropy(jnp.ones((3, 4, 10)), jnp.zeros((3, 4)), jnp.array([4, 2, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKLRJpCMpEQH"
      },
      "source": [
        "# Training\n",
        "\n",
        "We use teacher forcing, where the inputs to the decoder are \"bos\" (beginning of sentence), followed by the ground truth target tokens from the previous step, as shown below.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/probml/probml-notebooks/blob/main/images/seq2seq.png?raw=true\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0OJenK-mMh1m"
      },
      "outputs": [],
      "source": [
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        xlabel=None,\n",
        "        ylabel=None,\n",
        "        legend=None,\n",
        "        xlim=None,\n",
        "        ylim=None,\n",
        "        xscale=\"linear\",\n",
        "        yscale=\"linear\",\n",
        "        fmts=(\"-\", \"m--\", \"g-.\", \"r:\"),\n",
        "        nrows=1,\n",
        "        ncols=1,\n",
        "        figsize=(3.5, 2.5),\n",
        "    ):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        display.set_matplotlib_formats(\"svg\")\n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [\n",
        "                self.axes,\n",
        "            ]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return jnp.array(self.times).cumsum().tolist()\n",
        "\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "p2fqQdzkZvdF"
      },
      "outputs": [],
      "source": [
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_TV-1Pqbq6eB"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def grad_clipping(grads, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "\n",
        "    def grad_update(grads):\n",
        "        return jax.tree_map(lambda g: g * theta / norm, grads)\n",
        "\n",
        "    norm = jnp.sqrt(sum(jax.tree_util.tree_leaves(jax.tree_map(lambda x: jnp.sum(x**2), grads))))\n",
        "    # Update gradient if norm > theta\n",
        "    # This is jax.jit compatible\n",
        "    grads = jax.lax.cond(norm > theta, grad_update, lambda g: g, grads)\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BT8Myt8rO015"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, batch, bos_idx, rngs):\n",
        "    \"\"\"Train for a single step.\"\"\"\n",
        "    # Make sure to get a new RNG at every step\n",
        "    step = state.step\n",
        "    rngs = {name: jax.random.fold_in(rng, step) for name, rng in rngs.items()}\n",
        "\n",
        "    def loss_fn(params):\n",
        "        X, X_valid_len, Y, Y_valid_len = batch\n",
        "        bos = jnp.array([bos_idx] * Y.shape[0]).reshape(-1, 1)\n",
        "        dec_input = jnp.concatenate([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
        "        _, Y_hat = state.apply_fn({\"params\": params}, X, dec_input, X_valid_len, deterministic=False, rngs=rngs)\n",
        "        loss = masked_softmax_cross_entropy(Y_hat, Y, Y_valid_len)\n",
        "        return loss.sum()\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn)\n",
        "    loss, grads = grad_fn(state.params)\n",
        "    grads = grad_clipping(grads, 1)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "\n",
        "    return state, loss\n",
        "\n",
        "def train_seq2seq(state, data_iter, num_epochs, tgt_vocab, rngs):\n",
        "    \"\"\"Train a model for sequence to sequence.\"\"\"\n",
        "    animator = Animator(xlabel=\"epoch\", ylabel=\"loss\", xlim=[10, num_epochs])\n",
        "    for epoch in range(num_epochs):\n",
        "        timer = Timer()\n",
        "        metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "        for batch in data_iter:\n",
        "            state, loss = train_step(state, batch, tgt_vocab[\"<bos>\"], rngs)\n",
        "            num_tokens = batch[3].sum()\n",
        "            metric.add(loss, num_tokens)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
        "\n",
        "    device = jax.default_backend()\n",
        "    print(f\"loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} \" f\"tokens/sec on {device}\")\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c0jpCdjwRxEv"
      },
      "outputs": [],
      "source": [
        "def create_train_state(net, params, lr):\n",
        "    \"\"\"Create the train state for the network.\"\"\"\n",
        "    tx = optax.adam(learning_rate=lr)\n",
        "    state = train_state.TrainState.create(apply_fn=net.apply, params=params, tx=tx)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "JbntBH02R1Gf"
      },
      "outputs": [],
      "source": [
        "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
        "lr, num_epochs = 0.005, 300\n",
        "\n",
        "encoder = Seq2SeqEncoder(\n",
        "    vocab_size=len(src_vocab),\n",
        "    embed_size=embed_size,\n",
        "    num_hiddens=num_hiddens,\n",
        "    num_layers=num_layers,\n",
        "    dropout_rate=dropout,\n",
        ")\n",
        "decoder = Seq2SeqDecoder(\n",
        "    vocab_size=len(tgt_vocab),\n",
        "    embed_size=embed_size,\n",
        "    num_hiddens=num_hiddens,\n",
        "    num_layers=num_layers,\n",
        "    dropout_rate=dropout,\n",
        ")\n",
        "net = EncoderDecoder(encoder=encoder, decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqL5rbuRTHbC",
        "outputId": "626e1749-141e-4fdb-95eb-2a456ae88889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-25-3164811847.py:21: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  display.set_matplotlib_formats(\"svg\")\n",
            "/tmp/ipython-input-27-3121966513.py:8: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
            "  norm = jnp.sqrt(sum(jax.tree_util.tree_leaves(jax.tree_map(lambda x: jnp.sum(x**2), grads))))\n"
          ]
        }
      ],
      "source": [
        "params_rng, dropout_rng = jax.random.split(rng)\n",
        "rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n",
        "\n",
        "init_x = init_dec_input = jnp.ones((batch_size, num_steps), dtype=jnp.int32)\n",
        "init_x_valid_len = jnp.ones((batch_size,), dtype=jnp.int32)\n",
        "params = net.init(rngs, init_x, init_dec_input, init_x_valid_len)[\"params\"]\n",
        "\n",
        "state = create_train_state(net, params, lr)\n",
        "# Set the seed for the data iterator\n",
        "random.seed(0)\n",
        "state = train_seq2seq(state, train_iter, num_epochs, tgt_vocab, rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XSkOuMCr0yw"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "We use greedy decoding, where the inputs to the decoder are \"bos\" (beginning of sentence), followed by the most likely target token from the previous step, as shown below. We keep decoding until the model generates \"eos\" (end of sentence).\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/probml/probml-notebooks/blob/main/images/seq2seq-predict.png?raw=true\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2VB7oXlqPJD"
      },
      "outputs": [],
      "source": [
        "def predict_seq2seq(net, state, src_sentence, src_vocab, tgt_vocab, num_steps, save_attention_weights=False):\n",
        "    \"\"\"Predict for sequence to sequence.\"\"\"\n",
        "    src_tokens = src_vocab[src_sentence.lower().split(\" \")] + [src_vocab[\"<eos>\"]]\n",
        "    enc_valid_len = jnp.array([len(src_tokens)])\n",
        "    src_tokens = truncate_pad(src_tokens, num_steps, src_vocab[\"<pad>\"])\n",
        "    # Add the batch axis\n",
        "    enc_X = jnp.expand_dims(jnp.array(src_tokens, dtype=jnp.int32), axis=0)\n",
        "    enc_outputs = net.encoder.apply({\"params\": state.params[\"encoder\"]}, enc_X, enc_valid_len, deterministic=True)\n",
        "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
        "    # Add the batch axis\n",
        "    dec_X = jnp.expand_dims(jnp.array([tgt_vocab[\"<bos>\"]], dtype=jnp.int32), axis=0)\n",
        "    output_seq, attention_weight_seq = [], []\n",
        "    for _ in range(num_steps):\n",
        "        dec_state, Y = net.decoder.apply({\"params\": state.params[\"decoder\"]}, dec_state, dec_X, deterministic=True)\n",
        "        # We use the token with the highest prediction likelihood as the input\n",
        "        # of the decoder at the next time step\n",
        "        dec_X = Y.argmax(axis=2)\n",
        "        pred = dec_X.squeeze(axis=0).item()\n",
        "        # Save attention weights (to be covered later)\n",
        "        if save_attention_weights:\n",
        "            attention_weight_seq.append(net.decoder.attention_weights)\n",
        "        # Once the end-of-sequence token is predicted, the generation of the\n",
        "        # output sequence is complete\n",
        "        if pred == tgt_vocab[\"<eos>\"]:\n",
        "            break\n",
        "        output_seq.append(pred)\n",
        "    return \" \".join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vv9eQKjtud1"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-bjpplDtwee"
      },
      "source": [
        "In the MT community, the standard evaluation metric is known as BLEU (Bilingual Evaluation Understudy), which measures how many n-grams in the predicted target match the true label target.\n",
        "\n",
        "For example, suppose the prediction is A,B,B,C,D and the target is A,B,C,D,E,F.\n",
        "There are five 1-grams in the prediction, of which 4 find a match in the target (the second \"B\" is a \"false positive\"), so the precision for 1-grams is $p_1=4/5$. Similarly, there are four 2-grams, of which 3 find a match (the bigram \"BB\" does not occur), so $p_2 = 3/4$. We continue in this way to compute up to $p_k$, where $k$ is the max n-gram length. (Since we are using words, not characters, we typically keep $k$ small, to avoid sparse counts.)\n",
        "\n",
        "The BLEU score is then defined by\n",
        "$$\n",
        "\\exp(\\min(0, 1-\\frac{L_y}{L_p})) \\prod_{n=1}^k p_n^{0.5^n}\n",
        "$$\n",
        "where $L_y$ is the length of the target label sequence, and $L_p$ is the length of the prediction.\n",
        "\n",
        "Since predicting shorter sequences tends to give higher $p_n$ values, short sequences are penalized by the exponential factor. For example, suppose  $k=2$ and the label sequence  is A,B,C,D,E,F.\n",
        "If the predicted sequence is A,B,B,C,D,\n",
        "we have $p_1=4/5$ and $p_2=3/4$, and the penalty factor is\n",
        "$\\exp(1-6/5)=0.818$.\n",
        "If  the predicted sequence  is A,B,\n",
        "we have $p_1=p_2=1$, but the penalty factor is $\\exp(1−6/2)≈0.135$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwybiVMkwPp8"
      },
      "outputs": [],
      "source": [
        "def bleu(pred_seq, label_seq, k):\n",
        "    \"\"\"Compute the BLEU.\"\"\"\n",
        "    pred_tokens, label_tokens = pred_seq.split(\" \"), label_seq.split(\" \")\n",
        "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "    for n in range(1, k + 1):\n",
        "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "        for i in range(len_label - n + 1):\n",
        "            label_subs[\"\".join(label_tokens[i : i + n])] += 1\n",
        "        for i in range(len_pred - n + 1):\n",
        "            if label_subs[\"\".join(pred_tokens[i : i + n])] > 0:\n",
        "                num_matches += 1\n",
        "                label_subs[\"\".join(pred_tokens[i : i + n])] -= 1\n",
        "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Ov-JXBCkxCgp",
        "outputId": "37d7ff4c-8b71-422d-ac06-0dd671fa9e80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e509eb27-ccad-47c5-962a-40bc13a08edc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Truth</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Bleu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>go .</td>\n",
              "      <td>va !</td>\n",
              "      <td>va !</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i lost .</td>\n",
              "      <td>j'ai perdu .</td>\n",
              "      <td>j'ai perdu .</td>\n",
              "      <td>1.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>he's calm .</td>\n",
              "      <td>il est calme .</td>\n",
              "      <td>sois détendu .</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>i'm home .</td>\n",
              "      <td>je suis chez moi .</td>\n",
              "      <td>je suis certain .</td>\n",
              "      <td>0.51248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e509eb27-ccad-47c5-962a-40bc13a08edc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e509eb27-ccad-47c5-962a-40bc13a08edc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e509eb27-ccad-47c5-962a-40bc13a08edc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       English               Truth         Prediction     Bleu\n",
              "0         go .                va !               va !  1.00000\n",
              "1     i lost .        j'ai perdu .       j'ai perdu .  1.00000\n",
              "2  he's calm .      il est calme .     sois détendu .  0.00000\n",
              "3   i'm home .  je suis chez moi .  je suis certain .  0.51248"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prediction\n",
        "\n",
        "engs = [\"go .\", \"i lost .\", \"he's calm .\", \"i'm home .\"]\n",
        "fras = [\"va !\", \"j'ai perdu .\", \"il est calme .\", \"je suis chez moi .\"]\n",
        "\n",
        "data = []\n",
        "for eng, fra in zip(engs, fras):\n",
        "    translation, attention_weight_seq = predict_seq2seq(net, state, eng, src_vocab, tgt_vocab, num_steps)\n",
        "    score = bleu(translation, fra, k=2)\n",
        "    data.append((eng, fra, translation, score))\n",
        "df = pd.DataFrame.from_records(data, columns=[\"English\", \"Truth\", \"Prediction\", \"Bleu\"])\n",
        "display.display(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "nmt_jax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}