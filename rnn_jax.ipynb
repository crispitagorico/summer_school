{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCwSuMn6YVmk"
      },
      "source": [
        "# Recurrent neural networks\n",
        "\n",
        "We show how to implement RNNs from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o6VNlv_FYTbS"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp                      # JAX NumPy API for accelerated array operations\n",
        "import matplotlib.pyplot as plt                # Plotting library for creating figures\n",
        "import math                                   # Standard Python math functions (sqrt, sin, etc.)\n",
        "from IPython import display                   # Utilities for displaying plots and rich output in notebooks\n",
        "\n",
        "import jax                                    # Core JAX library for jit, vmap, random keys, etc.\n",
        "\n",
        "# Lazily install and import Flax's Linen API if not already available\n",
        "try:\n",
        "    import flax.linen as nn                   # Flax's neural network API for defining models\n",
        "except ModuleNotFoundError:\n",
        "    %pip install -qq flax                     # Quietly install Flax\n",
        "    import flax.linen as nn\n",
        "# Utilities for replicating/collecting parameters across devices\n",
        "from flax import jax_utils\n",
        "\n",
        "# Lazily install and import Optax optimizers if missing\n",
        "try:\n",
        "    import optax                               # Optax: gradient processing and optimization library\n",
        "except ModuleNotFoundError:\n",
        "    %pip install -qq optax                    # Quietly install Optax\n",
        "    import optax\n",
        "\n",
        "import collections                            # Container datatypes like deque, namedtuple, Counter\n",
        "import re                                     # Regular expressions for text processing\n",
        "import random                                 # Python's random module for non-JAX randomness\n",
        "import os                                     # File and directory operations\n",
        "import requests                               # HTTP library for downloading data\n",
        "import hashlib                                # Hashing algorithms (e.g., for file integrity checks)\n",
        "import time                                   # Time utilities for measuring durations\n",
        "import functools                              # Higher-order functions like partial, wraps\n",
        "\n",
        "# Ensure reproducibility for Python's random module\n",
        "random.seed(0)\n",
        "# Initialize JAX PRNG key for deterministic randomness in JAX operations\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "# Create a directory to save generated figures (no error if it already exists)\n",
        "os.makedirs('figures', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiYH4kGgak5S"
      },
      "source": [
        "# Data\n",
        "\n",
        " As data, we use the book \"The Time Machine\" by H G Wells,\n",
        "preprocessed using the code in [this colab](https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/text_preproc_torch.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tLIKRJomvBV5"
      },
      "outputs": [],
      "source": [
        "class SeqDataLoader:\n",
        "    \"\"\"\n",
        "    Data loader for sequential text data, yielding batches of subsequences for training language models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch_size : int\n",
        "        Number of sequences per batch.\n",
        "    num_steps : int\n",
        "        Length of each sequence chunk (number of time steps).\n",
        "    use_random_iter : bool\n",
        "        If True, sample sequence chunks randomly; otherwise, traverse sequentially.\n",
        "    max_tokens : int\n",
        "        Maximum number of tokens to include from the dataset.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    corpus : list of int\n",
        "        Flattened token ids from the loaded text corpus.\n",
        "    vocab : Vocab\n",
        "        Vocabulary mapping between tokens and indices.\n",
        "    data_iter_fn : callable\n",
        "        Iterator function: seq_data_iter_random or seq_data_iter_sequential.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "        # Choose the iteration function based on random vs. sequential sampling\n",
        "        if use_random_iter:\n",
        "            self.data_iter_fn = seq_data_iter_random\n",
        "        else:\n",
        "            self.data_iter_fn = seq_data_iter_sequential\n",
        "        # Load corpus and vocabulary from the Time Machine dataset (or similar)\n",
        "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
        "        # Store batch settings\n",
        "        self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Return an iterator over (X, Y) pairs, where:\n",
        "        - X has shape (batch_size, num_steps) of input token indices\n",
        "        - Y has the same shape, offset by one time step\n",
        "        \"\"\"\n",
        "        # Call the chosen iterator function with current corpus and settings\n",
        "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    \"\"\"\n",
        "    Vocabulary wrapper for mapping between tokens and numerical indices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokens : list of str, optional\n",
        "        List of tokens (e.g., characters or words) from which to build the vocabulary.\n",
        "    min_freq : int, default=0\n",
        "        Minimum frequency threshold; tokens appearing fewer times are excluded.\n",
        "    reserved_tokens : list of str, optional\n",
        "        Special tokens (e.g., '<pad>', '<bos>') that are always included.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        # Default to empty lists if none provided\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Count token frequencies in the corpus\n",
        "        counter = count_corpus(tokens)\n",
        "        # Sort tokens by descending frequency\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Reserve index 0 for unknown tokens\n",
        "        self.unk = 0\n",
        "        # Start building unique token list with '<unk>' and any reserved tokens\n",
        "        uniq_tokens = ['<unk>'] + reserved_tokens\n",
        "        # Add tokens meeting the frequency threshold, skipping duplicates\n",
        "        uniq_tokens += [token for token, freq in self.token_freqs\n",
        "                        if freq >= min_freq and token not in uniq_tokens]\n",
        "\n",
        "        # Create mappings: index to token and token to index\n",
        "        self.idx_to_token = []\n",
        "        self.token_to_idx = {}\n",
        "        for token in uniq_tokens:\n",
        "            self.idx_to_token.append(token)\n",
        "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the size of the vocabulary (number of unique tokens).\n",
        "        \"\"\"\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        \"\"\"\n",
        "        Convert token(s) to index/indices. Returns 0 (unk) if token not found.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokens : str or list of str\n",
        "            Single token or list of tokens to convert.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int or list of int\n",
        "            Corresponding index or list of indices.\n",
        "        \"\"\"\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            # Return index for single token, defaulting to unk\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        # Recursively process list of tokens\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        \"\"\"\n",
        "        Convert index/indices back to token(s).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indices : int or list of int\n",
        "            Single index or list of indices to convert.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        str or list of str\n",
        "            Corresponding token or list of tokens.\n",
        "        \"\"\"\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            # Return token for a single index\n",
        "            return self.idx_to_token[indices]\n",
        "        # Recursively process list of indices\n",
        "        return [self.idx_to_token[i] for i in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-TvZsL4gtHQ2"
      },
      "outputs": [],
      "source": [
        "def tokenize(lines, token=\"word\"):\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "    if token == \"word\":\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == \"char\":\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print(\"ERROR: unknown token type: \" + token)\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)\n",
        "\n",
        "\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n",
        "    # sequence\n",
        "    corpus = corpus[random.randint(0, num_steps - 1) :]\n",
        "    # Subtract 1 since we need to account for labels\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    # The starting indices for subsequences of length `num_steps`\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    # In random sampling, the subsequences from two adjacent random\n",
        "    # minibatches during iteration are not necessarily adjacent on the\n",
        "    # original sequence\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        # Return a sequence of length `num_steps` starting from `pos`\n",
        "        return corpus[pos : pos + num_steps]\n",
        "\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "        # Here, `initial_indices` contains randomized starting indices for\n",
        "        # subsequences\n",
        "        initial_indices_per_batch = initial_indices[i : i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield jnp.array(X), jnp.array(Y)\n",
        "\n",
        "\n",
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n",
        "    # Start with a random offset to partition a sequence\n",
        "    offset = random.randint(0, num_steps)\n",
        "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "    Xs = jnp.array(corpus[offset : offset + num_tokens])\n",
        "    Ys = jnp.array(corpus[offset + 1 : offset + 1 + num_tokens])\n",
        "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
        "    num_batches = Xs.shape[1] // num_steps\n",
        "    for i in range(0, num_steps * num_batches, num_steps):\n",
        "        X = Xs[:, i : i + num_steps]\n",
        "        Y = Ys[:, i : i + num_steps]\n",
        "        yield X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yRp-MH0Nv7rN"
      },
      "outputs": [],
      "source": [
        "def download(name, cache_dir=os.path.join(\"..\", \"data\")):\n",
        "    \"\"\"Download a file inserted into DATA_HUB, return the local filename.\"\"\"\n",
        "    assert name in DATA_HUB, f\"{name} does not exist in {DATA_HUB}.\"\n",
        "    url, sha1_hash = DATA_HUB[name]\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fname = os.path.join(cache_dir, url.split(\"/\")[-1])\n",
        "    if os.path.exists(fname):\n",
        "        sha1 = hashlib.sha1()\n",
        "        with open(fname, \"rb\") as f:\n",
        "            while True:\n",
        "                data = f.read(1048576)\n",
        "                if not data:\n",
        "                    break\n",
        "                sha1.update(data)\n",
        "        if sha1.hexdigest() == sha1_hash:\n",
        "            return fname  # Hit cache\n",
        "    print(f\"Downloading {fname} from {url}...\")\n",
        "    r = requests.get(url, stream=True, verify=True)\n",
        "    with open(fname, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    return fname\n",
        "\n",
        "\n",
        "def read_time_machine():\n",
        "    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n",
        "    with open(download(\"time_machine\"), \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub(\"[^A-Za-z]+\", \" \", line).strip().lower() for line in lines]\n",
        "\n",
        "\n",
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "    lines = read_time_machine()\n",
        "    tokens = tokenize(lines, \"char\")\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [vocab[token] for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "\n",
        "def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
        "    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n",
        "    return data_iter, data_iter.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG2T4maNYdsG",
        "outputId": "fa4b463a-c3c9-4ea0-b05b-cafe45a46036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
          ]
        }
      ],
      "source": [
        "DATA_HUB = dict()\n",
        "DATA_URL = \"http://d2l-data.s3-accelerate.amazonaws.com/\"\n",
        "DATA_HUB[\"time_machine\"] = (DATA_URL + \"timemachine.txt\", \"090b5e7e70c295757f55df93cb0a180b9691891a\")\n",
        "\n",
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGQryRwZauLq"
      },
      "source": [
        "# Model\n",
        "\n",
        "We fit an unconditional RNN, for language modeling (ie. not vec2seq or seq2seq. Following the D2L notation, the model has the form\n",
        "$$\\begin{align}\n",
        "H_t &= \\phi(X_t W_{xh} + H_{t-1}  W_{hh} + b_h) \\\\\n",
        "O_t &= H_t W_{hq} + b_q\n",
        "\\end{align}\n",
        "$$\n",
        "where $X_t$ is the $(n,d)$ matrix of (one-hot) inputs\n",
        "(for batch size $n$ and vocabulary size $d$),\n",
        "$H_t$ is the $(n,h)$ matrix of hidden states\n",
        "(for $h$ hidden states),\n",
        "and $O_t$ is the $(n,q)$ matrix of output logits\n",
        "(for $q$ output labels, often $q=d$).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3TC24W0EanF7"
      },
      "outputs": [],
      "source": [
        "# Create the initial parameters\n",
        "def get_params(vocab_size, num_hiddens, init_rng):\n",
        "    num_inputs = num_outputs = vocab_size\n",
        "\n",
        "    def normal(shape, rng):\n",
        "        return jax.random.normal(rng, shape=shape) * 0.01\n",
        "\n",
        "    hidden_rng, out_rng = jax.random.split(init_rng)\n",
        "    # Hidden layer parameters\n",
        "    W_xh = normal((num_inputs, num_hiddens), hidden_rng)\n",
        "    W_hh = normal((num_hiddens, num_hiddens), hidden_rng)\n",
        "    b_h = jnp.zeros(num_hiddens)\n",
        "    # Output layer parameters\n",
        "    W_hq = normal((num_hiddens, num_outputs), out_rng)\n",
        "    b_q = jnp.zeros(num_outputs)\n",
        "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dEf08b5ccaPx"
      },
      "outputs": [],
      "source": [
        "# Create the initial state\n",
        "# We assume this is a tuple of one element (later we will use longer tuples)\n",
        "def init_rnn_state(batch_size, num_hiddens):\n",
        "    return (jnp.zeros((batch_size, num_hiddens)),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NkvId2WmcqFz"
      },
      "outputs": [],
      "source": [
        "# Forward function.\n",
        "# Input sequence is (T,B,V), where T is length of the sequence, B is batch size, V is vocab size.\n",
        "# We iterate over each time step, and process the batch (for that timestep in parallel).\n",
        "# Output sequence is (T*B, V), since we concatenate all the time steps.\n",
        "# We also return the final state, so we can process the next subsequence.\n",
        "@jax.jit\n",
        "def rnn(params, state, inputs):\n",
        "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
        "    (H,) = state\n",
        "    outputs = []\n",
        "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "    for X in inputs:\n",
        "        H = jnp.tanh((X @ W_xh) + (H @ W_hh) + b_h)\n",
        "        Y = H @ W_hq + b_q\n",
        "        outputs.append(Y)\n",
        "    return jnp.concatenate(outputs, axis=0), (H,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qw3vhoRKdVxt"
      },
      "outputs": [],
      "source": [
        "# Make the model class\n",
        "# Input X to apply is (B,T) matrix of integers (from vocab encoding).\n",
        "# We transpose this to (T,B) then one-hot encode to (T,B,V), where V is vocab.\n",
        "# The result is passed to the forward function.\n",
        "# (We define the forward function as an argument, so we can change it later.)\n",
        "\n",
        "\n",
        "class RNNModelScratch:\n",
        "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, num_hiddens, get_params, init_state, forward_fn):\n",
        "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "        self.init_state, self.get_params = init_state, get_params\n",
        "        self.forward_fn = forward_fn\n",
        "\n",
        "    def apply(self, params, state, X):\n",
        "        X = jax.nn.one_hot(X.T, num_classes=self.vocab_size)\n",
        "        return self.forward_fn(params, state, X)\n",
        "\n",
        "    def begin_state(self, batch_size):\n",
        "        return self.init_state(batch_size, self.num_hiddens)\n",
        "\n",
        "    def init_params(self, rng):\n",
        "        return self.get_params(self.vocab_size, self.num_hiddens, rng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zJ2brHhpdbh5"
      },
      "outputs": [],
      "source": [
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, get_params, init_rnn_state, rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibw5knTqdxrv",
        "outputId": "90e3b33d-2e5c-4874-8ce0-3988bf207401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "(2, 512)\n",
            "28\n",
            "(10, 28)\n"
          ]
        }
      ],
      "source": [
        "X = jnp.arange(10).reshape((2, 5))  # batch 2, sequence length is 5\n",
        "params = net.init_params(rng)\n",
        "state = net.begin_state(X.shape[0])\n",
        "print(len(state))  # length 1\n",
        "print(state[0].shape)  # (2,512)\n",
        "\n",
        "Y, new_state = net.apply(params, state, X)\n",
        "print(len(vocab))  # 28\n",
        "print(Y.shape)  # (2x5, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLtX5YfSfiBn"
      },
      "source": [
        "# Prediction (generation)\n",
        "\n",
        "We pass in an initial prefix string, that is not generated. This is used to \"warm-up\" the hidden state. Specifically, we update the hidden state given the observed prefix, but don't generate anything. After that, for each of the T steps, we compute the (1,V) output tensor, pick the argmax index, and append it to the output. Finally, we convert the to indices to readable token sequence of size (1,T). (Note that this is a greedy, deterministic procedure.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MvGhZUIGd3F3"
      },
      "outputs": [],
      "source": [
        "def predict(prefix, num_preds, net, params, vocab):\n",
        "    \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "    state = net.begin_state(batch_size=1)\n",
        "    outputs = [vocab[prefix[0]]]\n",
        "    get_input = lambda: jnp.array([outputs[-1]]).reshape((1, 1))\n",
        "    for y in prefix[1:]:  # Warm-up period\n",
        "        _, state = net.apply(params, state, get_input())\n",
        "        outputs.append(vocab[y])\n",
        "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
        "        y, state = net.apply(params, state, get_input())\n",
        "        y = y.reshape(-1, y.shape[-1])\n",
        "        outputs.append(int(y.argmax(axis=1).item()))\n",
        "    return \"\".join([vocab.idx_to_token[i] for i in outputs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "678bHt1ogoh8",
        "outputId": "dd2eda88-8e11-41a7-eda4-fc34984521ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time traveller tig tig ti'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# sample 10 characters after the prefix.\n",
        "# since the model is untrained, the results will be garbage.\n",
        "predict(\"time traveller \", 10, net, params, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WTrDGSIhQxN"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPuo8za_hUWL"
      },
      "source": [
        "To ensure the gradient doesn't blow up when doing backpropagation through many layers, we use gradient clipping, which corresponds to the update\n",
        "$$\n",
        "g := \\min(1, \\theta /||g||) g\n",
        "$$\n",
        "where $\\theta$ is the scaling parameter, and $g$ is the gradient vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LDuvumDhgpvh"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def grad_clipping(grads, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "\n",
        "    def grad_update(grads):\n",
        "        return jax.tree_map(lambda g: g * theta / norm, grads)\n",
        "\n",
        "    norm = jnp.sqrt(sum(jax.tree_util.tree_leaves(jax.tree_map(lambda x: jnp.sum(x**2), grads))))\n",
        "    # Update gradient if norm > theta\n",
        "    # This is jax.jit compatible\n",
        "    grads = jax.lax.cond(norm > theta, grad_update, lambda g: g, grads)\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Exq_Lihxld"
      },
      "source": [
        "The training step is fairly standard, except for the use of gradient clipping, and the issue of the hidden state.\n",
        "If the data iterator uses random ordering of the sequences, we need to initialize the hidden state for each minibatch. However, if the data iterator uses sequential ordering, we only initialize the hidden state at the very beginning of the process. In the latter case, the hidden state will depend on the value at the previous minibatch.\n",
        "\n",
        "The state vector may be a tensor or a tuple, depending on what kind of RNN we are using. In addition, the parameter updater can be an optax optimizer, or a simpler custom sgd optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tA_QeZH5rp1o"
      },
      "outputs": [],
      "source": [
        "class Animator:\n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        xlabel=None,\n",
        "        ylabel=None,\n",
        "        legend=None,\n",
        "        xlim=None,\n",
        "        ylim=None,\n",
        "        xscale=\"linear\",\n",
        "        yscale=\"linear\",\n",
        "        fmts=(\"-\", \"m--\", \"g-.\", \"r:\"),\n",
        "        nrows=1,\n",
        "        ncols=1,\n",
        "        figsize=(3.5, 2.5),\n",
        "    ):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        display.set_matplotlib_formats(\"svg\")\n",
        "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [\n",
        "                self.axes,\n",
        "            ]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: set_axes(self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "\n",
        "class Timer:\n",
        "    \"\"\"Record multiple running times.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.times = []\n",
        "        self.start()\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Start the timer.\"\"\"\n",
        "        self.tik = time.time()\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
        "        self.times.append(time.time() - self.tik)\n",
        "        return self.times[-1]\n",
        "\n",
        "    def avg(self):\n",
        "        \"\"\"Return the average time.\"\"\"\n",
        "        return sum(self.times) / len(self.times)\n",
        "\n",
        "    def sum(self):\n",
        "        \"\"\"Return the sum of time.\"\"\"\n",
        "        return sum(self.times)\n",
        "\n",
        "    def cumsum(self):\n",
        "        \"\"\"Return the accumulated time.\"\"\"\n",
        "        return jnp.array(self.times).cumsum().tolist()\n",
        "\n",
        "\n",
        "class Accumulator:\n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def sgd(params, grads, lr, batch_size):\n",
        "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
        "    params = jax.tree_map(lambda p, g: p - lr * g / batch_size, params, grads)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Tx-RsQNtEihe"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(apply_fn, loss_fn, params, state, X, Y):\n",
        "    def loss(params, state, X, Y):\n",
        "        y = Y.T.reshape(-1)  # (B,T) -> (T,B)\n",
        "        y_hat, state = apply_fn(params, state, X)\n",
        "        y_hat = y_hat.reshape(-1, y_hat.shape[-1])\n",
        "        y_one_hot = jax.nn.one_hot(y, num_classes=y_hat.shape[-1])\n",
        "        return loss_fn(y_hat, y_one_hot).mean(), state\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss, has_aux=True)\n",
        "    (l, state), grads = grad_fn(params, state, X, Y)\n",
        "    grads = grad_clipping(grads, 1)\n",
        "    return l, state, grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Eq_QMLUKhw3k"
      },
      "outputs": [],
      "source": [
        "def train_epoch(net, params, train_iter, loss, updater, use_random_iter):\n",
        "    state, timer = None, Timer()\n",
        "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
        "    if isinstance(updater, optax.GradientTransformation):\n",
        "        updater_state = updater.init(params)\n",
        "    # Convert to jax Partial functions for jax.jit compatibility\n",
        "    apply_fn = jax.tree_util.Partial(net.apply)\n",
        "    loss_fn = jax.tree_util.Partial(loss)\n",
        "    for X, Y in train_iter:\n",
        "        if state is None or use_random_iter:\n",
        "            # Initialize `state` when either it is the first iteration or\n",
        "            # using random sampling\n",
        "            state = net.begin_state(batch_size=X.shape[0])\n",
        "        l, state, grads = train_step(apply_fn, loss_fn, params, state, X, Y)\n",
        "        if isinstance(updater, optax.GradientTransformation):\n",
        "            updates, updater_state = updater.update(grads, updater_state)\n",
        "            params = optax.apply_updates(params, updates)\n",
        "        else:\n",
        "            # batch_size=1 since the `mean` function has been invoked\n",
        "            params = updater(params, grads, batch_size=1)\n",
        "        metric.add(l * Y.size, Y.size)\n",
        "    return params, math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T64jNH0Ciq4o"
      },
      "source": [
        "The main training function is fairly standard.\n",
        "The loss function is per-symbol cross-entropy, $-\\log q(x_t)$, where $q$ is the model prediction from the RNN. Since we compute the average loss across time steps within a batch, we are computing $-\\frac{1}{T} \\sum_{t=1}^T \\log p(x_t|x_{1:t-1})$. The exponential of this is the perplexity (ppl). We plot this metric during training, since it is independent of document length. In addition, we print the MAP sequence prediction following the suffix 'time traveller', to get a sense of what the model is doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0tPGxQDWiqfl"
      },
      "outputs": [],
      "source": [
        "def train(net, params, train_iter, vocab, lr, num_epochs, use_random_iter=False):\n",
        "    loss = optax.softmax_cross_entropy\n",
        "    animator = Animator(xlabel=\"epoch\", ylabel=\"perplexity\", legend=[\"train\"], xlim=[10, num_epochs])\n",
        "    # Initialize\n",
        "    if isinstance(net, nn.Module):\n",
        "        updater = optax.sgd(lr)\n",
        "    else:\n",
        "        updater = lambda params, grads, batch_size: sgd(params, grads, lr, batch_size)\n",
        "    num_preds = 50\n",
        "    predict_ = lambda prefix: predict(prefix, num_preds, net, params, vocab)\n",
        "    # Train and predict\n",
        "    for epoch in range(num_epochs):\n",
        "        params, ppl, speed = train_epoch(net, params, train_iter, loss, updater, use_random_iter)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            # Prediction takes time on the flax model\n",
        "            # print(predict_('time traveller'))\n",
        "            animator.add(epoch + 1, [ppl])\n",
        "    device = jax.default_backend()\n",
        "    print(f\"perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {device}\")\n",
        "    print(predict_(\"time traveller\"))\n",
        "    print(predict_(\"traveller\"))\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "OfD7J9-YjSCi",
        "outputId": "e2d36679-6bc9-4eae-e16c-504d71f400dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity 1.0, 82998.5 tokens/sec on gpu\n",
            "time travelleryou can show black is white by argument said filby\n",
            "travelleryou can show black is white by argument said filby\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-06-24T19:11:42.787918</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 262.1875 183.35625 \nL 262.1875 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 145.8 \nL 245.44375 145.8 \nL 245.44375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 86.015179 145.8 \nL 86.015179 7.2 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"md7a7f91518\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#md7a7f91518\" x=\"86.015179\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 100 -->\n      <g transform=\"translate(76.471429 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 125.872321 145.8 \nL 125.872321 7.2 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#md7a7f91518\" x=\"125.872321\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(116.328571 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 165.729464 145.8 \nL 165.729464 7.2 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#md7a7f91518\" x=\"165.729464\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 300 -->\n      <g transform=\"translate(156.185714 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 205.586607 145.8 \nL 205.586607 7.2 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#md7a7f91518\" x=\"205.586607\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 400 -->\n      <g transform=\"translate(196.042857 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 245.44375 145.8 \nL 245.44375 7.2 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#md7a7f91518\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 500 -->\n      <g transform=\"translate(235.9 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <g transform=\"translate(132.565625 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(61.523438 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(125 0)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(186.181641 0)\"/>\n      <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(241.162109 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 50.14375 124.58488 \nL 245.44375 124.58488 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"m1cdde5cdee\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1cdde5cdee\" x=\"50.14375\" y=\"124.58488\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2.5 -->\n      <g transform=\"translate(27.240625 128.384099) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 50.14375 99.172894 \nL 245.44375 99.172894 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m1cdde5cdee\" x=\"50.14375\" y=\"99.172894\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5.0 -->\n      <g transform=\"translate(27.240625 102.972112) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 50.14375 73.760908 \nL 245.44375 73.760908 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m1cdde5cdee\" x=\"50.14375\" y=\"73.760908\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 7.5 -->\n      <g transform=\"translate(27.240625 77.560126) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(95.410156 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <path d=\"M 50.14375 48.348921 \nL 245.44375 48.348921 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use xlink:href=\"#m1cdde5cdee\" x=\"50.14375\" y=\"48.348921\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10.0 -->\n      <g transform=\"translate(20.878125 52.14814) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(127.246094 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(159.033203 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_19\">\n      <path d=\"M 50.14375 22.936935 \nL 245.44375 22.936935 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use xlink:href=\"#m1cdde5cdee\" x=\"50.14375\" y=\"22.936935\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 12.5 -->\n      <g transform=\"translate(20.878125 26.736154) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" transform=\"translate(127.246094 0)\"/>\n       <use xlink:href=\"#DejaVuSans-35\" transform=\"translate(159.033203 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- perplexity -->\n     <g transform=\"translate(14.798437 101.626563) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(63.476562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(125 0)\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(166.113281 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(229.589844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(257.373047 0)\"/>\n      <use xlink:href=\"#DejaVuSans-78\" transform=\"translate(317.146484 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(376.326172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(404.109375 0)\"/>\n      <use xlink:href=\"#DejaVuSans-79\" transform=\"translate(443.318359 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 50.14375 13.5 \nL 54.129464 41.450857 \nL 58.115179 52.868132 \nL 62.100893 58.883366 \nL 66.086607 62.894626 \nL 70.072321 67.109547 \nL 74.058036 69.110128 \nL 78.04375 72.239851 \nL 82.029464 72.799672 \nL 86.015179 74.957069 \nL 90.000893 78.098566 \nL 93.986607 80.921955 \nL 97.972321 83.343873 \nL 101.958036 88.972839 \nL 105.94375 89.41293 \nL 109.929464 94.938194 \nL 113.915179 99.592801 \nL 117.900893 105.425894 \nL 121.886607 112.414061 \nL 125.872321 118.380566 \nL 129.858036 122.713548 \nL 133.84375 125.375276 \nL 137.829464 129.247101 \nL 141.815179 131.275609 \nL 145.800893 133.645998 \nL 149.786607 135.035211 \nL 153.772321 135.364648 \nL 157.758036 135.883662 \nL 161.74375 136.495949 \nL 165.729464 136.984468 \nL 169.715179 137.205623 \nL 173.700893 137.817618 \nL 177.686607 138.300341 \nL 181.672321 138.923288 \nL 185.658036 138.934167 \nL 189.64375 138.967462 \nL 193.629464 139.128313 \nL 197.615179 139.392269 \nL 201.600893 139.236076 \nL 205.586607 139.366164 \nL 209.572321 139.5 \nL 213.558036 139.398177 \nL 217.54375 139.267226 \nL 221.529464 139.297338 \nL 225.515179 139.377434 \nL 229.500893 137.909628 \nL 233.486607 138.285219 \nL 237.472321 138.936256 \nL 241.458036 139.430451 \nL 245.44375 139.450731 \n\" clip-path=\"url(#pb6110a3391)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 145.8 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 245.44375 145.8 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 145.8 \nL 245.44375 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 245.44375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 183.16875 29.878125 \nL 238.44375 29.878125 \nQ 240.44375 29.878125 240.44375 27.878125 \nL 240.44375 14.2 \nQ 240.44375 12.2 238.44375 12.2 \nL 183.16875 12.2 \nQ 181.16875 12.2 181.16875 14.2 \nL 181.16875 27.878125 \nQ 181.16875 29.878125 183.16875 29.878125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 185.16875 20.298438 \nL 195.16875 20.298438 \nL 205.16875 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- train -->\n     <g transform=\"translate(213.16875 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb6110a3391\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "random.seed(0)\n",
        "num_epochs, lr = 500, 1\n",
        "num_hiddens = 512\n",
        "net = RNNModelScratch(len(vocab), num_hiddens, get_params, init_rnn_state, rnn)\n",
        "params = net.init_params(rng)\n",
        "params = train(net, params, train_iter, vocab, lr, num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ3nTS2flVtH",
        "outputId": "de6f4c99-572d-4d99-8372-2586da49c688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time traveller <unk>other wh has a grmed and you smit react bat thing to the end of our livesthat said a very young man \n",
            "the german scholars have improved greek so muchthen there is the future said the very young man just th\n"
          ]
        }
      ],
      "source": [
        "num_preds = 100\n",
        "predict_ = lambda prefix: predict(prefix, num_preds, net, params, vocab)\n",
        "\n",
        "print(predict_(\"time traveller \\n\"))\n",
        "print(predict_(\"the\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0TXLX21mJ_I",
        "outputId": "460fea6b-d0cf-434f-93cd-82e06e78eb06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the german scholars have improved greek so muchthen there is the future said the very young man just thinkone might invest all one s money leave it to accumulate atinterest and hurry on aheadto discover a society said i erected on a strictly communisticbasisof all the wild extravagant te tam and we berale in time as we move about in the other dimensions of spacethere that uleatable har and aid aid firimedyci venely have an ansugo ac aldid you gimentyorgeis thes sothe sothuurd arectes ol shad ther h\n"
          ]
        }
      ],
      "source": [
        "num_preds = 500\n",
        "predict_ = lambda prefix: predict(prefix, num_preds, net, params, vocab)\n",
        "\n",
        "print(predict_(\"the\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URvl3gPSvy2P"
      },
      "source": [
        "# Creating a Flax module\n",
        "\n",
        "We now show how to use create an RNN as a module, which is faster than our pure Python implementation.\n",
        "\n",
        "While Flax has cells for more advanced recurrent models, it does not have a basic RNNCell. Therefore, we create an RNNCell similar to those defined in `flax.linen.recurrent` [here](https://flax.readthedocs.io/en/latest/_modules/flax/linen/recurrent.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "v42JrpEzIm-y"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Tuple\n",
        "\n",
        "PRNGKey = Any\n",
        "Shape = Tuple[int]\n",
        "Dtype = Any\n",
        "Array = Any\n",
        "\n",
        "\n",
        "class RNNCell(nn.recurrent.RNNCellBase):\n",
        "    \"\"\"RNN Cell.\"\"\"\n",
        "\n",
        "    activation_fn: Callable[..., Any] = nn.activation.tanh\n",
        "    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.linear.default_kernel_init\n",
        "    recurrent_kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.orthogonal()\n",
        "    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = nn.initializers.zeros\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, carry, inputs):\n",
        "        \"\"\"RNN Cell.\n",
        "\n",
        "        Args:\n",
        "            carry: the hidden state of the RNN cell,\n",
        "                initialized using `RNNCell.initialize_carry`.\n",
        "            inputs: an ndarray with the input for the current time step.\n",
        "                All dimensions except the final are considered batch dimensions.\n",
        "\n",
        "        Returns:\n",
        "            A tuple with the new carry and the output.\n",
        "        \"\"\"\n",
        "        h = carry\n",
        "        hidden_features = h.shape[-1]\n",
        "        # Dense layer applied to the previous state\n",
        "        dense_h = functools.partial(\n",
        "            nn.Dense,\n",
        "            features=hidden_features,\n",
        "            use_bias=False,\n",
        "            kernel_init=self.recurrent_kernel_init,\n",
        "            bias_init=self.bias_init,\n",
        "        )\n",
        "        # Dense layer applied to the input, i\n",
        "        dense_i = functools.partial(\n",
        "            nn.Dense, features=hidden_features, use_bias=True, kernel_init=self.kernel_init, bias_init=self.bias_init\n",
        "        )\n",
        "        new_h = self.activation_fn(dense_i()(inputs) + dense_h()(h))\n",
        "        return new_h, new_h\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize_carry(rng, batch_dims, size, init_fn=nn.initializers.zeros):\n",
        "        \"\"\"Initialize the RNN cell carry.\n",
        "        Args:\n",
        "            rng: random number generator passed to the init_fn.\n",
        "            batch_dims: a tuple providing the shape of the batch dimensions.\n",
        "            size: the size or number of features of the memory.\n",
        "            init_fn: initializer function for the carry.\n",
        "        Returns:\n",
        "            An initialized carry for the given RNN cell.\n",
        "        \"\"\"\n",
        "        mem_shape = batch_dims + (size,)\n",
        "        return init_fn(rng, mem_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqn-ThsOLEnC"
      },
      "source": [
        "Now, we create an RNN module to call the RNNCell for each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SzIM0zW2OT7Q"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    @functools.partial(\n",
        "        nn.transforms.scan, variable_broadcast=\"params\", in_axes=0, out_axes=0, split_rngs={\"params\": False}\n",
        "    )\n",
        "    @nn.compact\n",
        "    def __call__(self, state, x):\n",
        "        return RNNCell()(state, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def initialize_carry(rng, batch_dims, size):\n",
        "        return RNNCell.initialize_carry(rng, batch_dims, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbe7sYAsAjr",
        "outputId": "dd8231d2-59f6-4394-fa62-b4467b01d7ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 256)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "num_hiddens = 256\n",
        "rnn_layer = RNN()\n",
        "\n",
        "batch_size, num_steps = 32, 35\n",
        "state = rnn_layer.initialize_carry(rng, (batch_size,), num_hiddens)\n",
        "state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1hVhsSgwLLU"
      },
      "source": [
        "Now we update the state with a random one-hot array of inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pAV6m9rwN1d",
        "outputId": "c5759dcd-27af-47bb-a541-95423281dafa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35, 32, 256), (32, 256))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X = jax.random.normal(rng, shape=(num_steps, batch_size, len(vocab)))\n",
        "params = rnn_layer.init(rng, state, X)\n",
        "state_new, Y = rnn_layer.apply(params, state, X)\n",
        "Y.shape, state_new.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J9R2aHzwhK_"
      },
      "source": [
        "Now we define our model. It consists of an RNN Layer followed by a dense layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "89IqhaoSwiRT"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"The RNN model.\"\"\"\n",
        "\n",
        "    rnn: nn.Module\n",
        "    vocab_size: int\n",
        "    num_hiddens: int\n",
        "    bidirectional: bool = False\n",
        "\n",
        "    def setup(self):\n",
        "        # If the RNN is bidirectional (to be introduced later),\n",
        "        # `num_directions` should be 2, else it should be 1.\n",
        "        if not self.bidirectional:\n",
        "            self.num_directions = 1\n",
        "        else:\n",
        "            self.num_directions = 2\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, state, inputs):\n",
        "        X = jax.nn.one_hot(inputs.T, num_classes=self.vocab_size)\n",
        "        state, Y = self.rnn(state, X)\n",
        "        output = nn.Dense(self.vocab_size)(Y)\n",
        "        return output, state\n",
        "\n",
        "    def begin_state(self, batch_size=1):\n",
        "        # Use fixed random key since default state init fn is just `zeros`.\n",
        "        return self.rnn.initialize_carry(jax.random.PRNGKey(0), (batch_size,), num_hiddens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVO0IVrGxBD1"
      },
      "source": [
        "Test the untrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J19s-kDjwi3F",
        "outputId": "1af7389e-6603-47bf-d43c-af7076b55a80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time travellermcudgdqmiuxmcqlntdpcrslgyuamiqigtjphfasbnwhelnllat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "net = RNNModel(rnn=rnn_layer, vocab_size=len(vocab), num_hiddens=num_hiddens)\n",
        "params = net.init(rng, state, jnp.ones([batch_size, num_steps]))\n",
        "predict(\"time traveller\", 50, net, params, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6RGAFRrxIMl"
      },
      "source": [
        "Train it. The results are similar to the 'from scratch' implementation, but much faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "O_mu4sc2xEQH",
        "outputId": "652587dc-3b11-4a8a-a27d-151d65545a00"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"246.284375pt\" height=\"183.35625pt\" viewBox=\"0 0 246.284375 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-06-24T19:13:07.750911</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.10.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 183.35625 \nL 246.284375 183.35625 \nL 246.284375 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 34.240625 145.8 \nL 229.540625 145.8 \nL 229.540625 7.2 \nL 34.240625 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 70.112054 145.8 \nL 70.112054 7.2 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"me62bf05dc3\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#me62bf05dc3\" x=\"70.112054\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 100 -->\n      <g transform=\"translate(60.568304 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 109.969196 145.8 \nL 109.969196 7.2 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#me62bf05dc3\" x=\"109.969196\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <g transform=\"translate(100.425446 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 149.826339 145.8 \nL 149.826339 7.2 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#me62bf05dc3\" x=\"149.826339\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 300 -->\n      <g transform=\"translate(140.282589 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 189.683482 145.8 \nL 189.683482 7.2 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#me62bf05dc3\" x=\"189.683482\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 400 -->\n      <g transform=\"translate(180.139732 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path d=\"M 229.540625 145.8 \nL 229.540625 7.2 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#me62bf05dc3\" x=\"229.540625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 500 -->\n      <g transform=\"translate(219.996875 160.398438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(63.623047 0)\"/>\n       <use xlink:href=\"#DejaVuSans-30\" transform=\"translate(127.246094 0)\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- epoch -->\n     <g transform=\"translate(116.6625 174.076563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(61.523438 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" transform=\"translate(125 0)\"/>\n      <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(186.181641 0)\"/>\n      <use xlink:href=\"#DejaVuSans-68\" transform=\"translate(241.162109 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_11\">\n      <path d=\"M 34.240625 139.843934 \nL 229.540625 139.843934 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <defs>\n       <path id=\"m1673080702\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m1673080702\" x=\"34.240625\" y=\"139.843934\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 2 -->\n      <g transform=\"translate(20.878125 143.643152) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_13\">\n      <path d=\"M 34.240625 94.159377 \nL 229.540625 94.159377 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m1673080702\" x=\"34.240625\" y=\"94.159377\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 4 -->\n      <g transform=\"translate(20.878125 97.958596) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_15\">\n      <path d=\"M 34.240625 48.474821 \nL 229.540625 48.474821 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m1673080702\" x=\"34.240625\" y=\"48.474821\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 6 -->\n      <g transform=\"translate(20.878125 52.274039) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- perplexity -->\n     <g transform=\"translate(14.798438 101.626563) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \nL 2247 1797 \nL 3578 0 \nL 2900 0 \nL 1881 1375 \nL 863 0 \nL 184 0 \nL 1544 1831 \nL 300 3500 \nL 978 3500 \nL 1906 2253 \nL 2834 3500 \nL 3513 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(63.476562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(125 0)\"/>\n      <use xlink:href=\"#DejaVuSans-70\" transform=\"translate(166.113281 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" transform=\"translate(229.589844 0)\"/>\n      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(257.373047 0)\"/>\n      <use xlink:href=\"#DejaVuSans-78\" transform=\"translate(317.146484 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(376.326172 0)\"/>\n      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(404.109375 0)\"/>\n      <use xlink:href=\"#DejaVuSans-79\" transform=\"translate(443.318359 0)\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 34.240625 13.5 \nL 38.226339 47.789232 \nL 42.212054 81.102438 \nL 46.197768 98.876044 \nL 50.183482 109.614375 \nL 54.169196 121.438414 \nL 58.154911 125.218704 \nL 62.140625 130.194956 \nL 66.126339 136.411765 \nL 70.112054 139.5 \n\" clip-path=\"url(#pd1446f91b2)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 34.240625 145.8 \nL 34.240625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 229.540625 145.8 \nL 229.540625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 34.240625 145.8 \nL 229.540625 145.8 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 34.240625 7.2 \nL 229.540625 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 167.265625 29.878125 \nL 222.540625 29.878125 \nQ 224.540625 29.878125 224.540625 27.878125 \nL 224.540625 14.2 \nQ 224.540625 12.2 222.540625 12.2 \nL 167.265625 12.2 \nQ 165.265625 12.2 165.265625 14.2 \nL 165.265625 27.878125 \nQ 165.265625 29.878125 167.265625 29.878125 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 169.265625 20.298438 \nL 179.265625 20.298438 \nL 189.265625 20.298438 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- train -->\n     <g transform=\"translate(197.265625 23.798438) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(39.208984 0)\"/>\n      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(80.322266 0)\"/>\n      <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(141.601562 0)\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" transform=\"translate(169.384766 0)\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd1446f91b2\">\n   <rect x=\"34.240625\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "num_epochs, lr = 500, 1\n",
        "params = train(net, params, train_iter, vocab, lr, num_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "rnn_jax.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}